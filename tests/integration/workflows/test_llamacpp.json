{
  "name": "integration_test_llamacpp",
  "description": "Integration test for llama.cpp inference engine with GGUF gemma-3-4b-it",
  "data_dir": "tests/integration/data",
  "output_dir": "output/integration_tests",
  "prompts_dir": "src/polysome/templates/prompts",
  "log_dir": "output/integration_tests/logs",
  "workflow_settings": {
    "optimize_for_engines": false,
    "comment": "Integration test - llama.cpp engine"
  },
  "nodes": [
    {
      "id": "load_test_data",
      "type": "load",
      "params": {
        "name": "load_questions",
        "input_data_path": "test_questions.json",
        "primary_key": "id",
        "output_file_name": "loaded_questions.jsonl",
        "resume": true
      },
      "dependencies": []
    },
    {
      "id": "generate_answers_llamacpp",
      "type": "text_prompt",
      "params": {
        "name": "simple_qa",
        "template_context_map": {},
        "system_prompt_file": "system_prompt.txt",
        "user_prompt_file": "user_prompt.txt",
        "few_shot_lines_file": "few_shot.jsonl",
        "num_few_shots": 2,
        "model_name": "models/gemma-3-4b-it-q4_0.gguf",
        "inference_engine": "llama_cpp",
        "engine_options": {
          "chat_format": "gemma",
          "n_gpu_layers": -1,
          "n_ctx": 4096,
          "verbose": false
        },
        "generation_options": {
          "max_tokens": 256,
          "temperature": 0.7,
          "top_p": 0.9
        },
        "batch_size": 5,
        "output_data_attribute": "answer",
        "resume": true
      },
      "dependencies": ["load_test_data"]
    }
  ]
}
