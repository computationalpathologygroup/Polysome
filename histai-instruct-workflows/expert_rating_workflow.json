{
  "name": "expert_rating_workflow",
  "description": "Single node workflow for generating expert ratings using llm_expert_rating_gen_data prompts",
  "data_dir": "/data",
  "output_dir": "/output",
  "prompts_dir": "/prompts",
  "log_dir": "/output/logs",
  "workflow_settings": {
    "optimize_for_engines": true,
    "comment": "Expert rating generation workflow using vLLM"
  },
  "nodes": [
    {
      "id": "step0_jsonl_loader",
      "type": "load",
      "params": {
        "name": "load_expert_rating_data",
        "input_data_path": "processed_data_20250731_162648_with_primary_key.jsonl",
        "primary_key": "primary_key",
        "output_file_name": "expert_rating_workflow_input.jsonl",
        "resume": true
      },
      "dependencies": []
    },
    {
      "id": "expert_rating_node",
      "type": "text_prompt",
      "params": {
        "name": "llm_expert_rating_gen_data",
        "template_context_map": {},
        "system_prompt_file": "system_prompt.txt",
        "user_prompt_file": "user_prompt.txt",
        "few_shot_lines_file": "few_shot.jsonl",
        "num_few_shots": 0,
        "model_name": "/models/gemma-3-27b-it-quantized.w4a16",
        "inference_engine": "vllm_dp",
        "engine_options": {
          "data_parallel_size": 8,
          "gpus_per_dp_rank": 1,
          "trust_remote_code": true,
          "gpu_memory_utilization": 0.98,
          "max_model_len": 8192
        },
        "generation_options": {
          "max_tokens": 1024,
          "temperature": 0.7,
          "top_p": 0.9
        },
        "batch_size": 200,
        "output_data_attribute": "expert_rating",
        "resume": true,
        "parse_json": true,
        "use_shared_engines": true
      },
      "dependencies": [
        "step0_jsonl_loader"
      ]
    }
  ]
}
