{
  "name": "regenerate_questions_workflow",
  "description": "Single node workflow for regenerating questions using regenerate_questions prompts",
  "data_dir": "/data",
  "output_dir": "/output",
  "prompts_dir": "/prompts",
  "log_dir": "/output/logs",
  "workflow_settings": {
    "optimize_for_engines": true,
    "comment": "Question regeneration workflow using vLLM"
  },
  "nodes": [
    {
      "id": "step0_csv_loader",
      "type": "load",
      "params": {
        "name": "load_questions_data",
        "input_data_path": "threshold_questions_simple_20250803_142637.jsonl",
        "primary_key": "id",
        "output_file_name": "regenerate_questions_workflow_input.jsonl",
        "resume": true
      },
      "dependencies": []
    },
    {
      "id": "regenerate_questions_node",
      "type": "text_prompt",
      "params": {
        "name": "regenerate_questions",
        "template_context_map": {},
        "system_prompt_file": "system_prompt.txt",
        "user_prompt_file": "user_prompt.txt",
        "few_shot_lines_file": "few_shot.jsonl",
        "num_few_shots": 0,
        "model_name": "/models/gemma-3-27b-it-quantized.w4a16",
        "inference_engine": "vllm_dp",
        "engine_options": {
          "data_parallel_size": 8,
          "gpus_per_dp_rank": 1,
          "trust_remote_code": true,
          "gpu_memory_utilization": 0.98,
          "max_model_len": 2048
        },
        "generation_options": {
          "max_tokens": 768,
          "temperature": 0.7,
          "top_p": 0.9
        },
        "batch_size": 480,
        "output_data_attribute": "regenerated_questions",
        "resume": true,
        "parse_json": true,
        "use_shared_engines": true
      },
      "dependencies": [
        "step0_csv_loader"
      ]
    }
  ]
}
